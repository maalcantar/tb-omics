{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTSTANDING ISSUES: THERE ARE DUPLICATE METABOLITES LOL \n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import collections\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import statsmodels.stats.multitest as multi\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "sns.set_style('white')\n",
    "pd.options.display.float_format = '{:,.7f}'.format\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "\n",
    "#set for reproducibility\n",
    "#check current working directory\n",
    "\n",
    "if (os.getcwd().split('\\\\')[-1] != 'data'):\n",
    "    os.chdir('.\\data')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metabolomics(filename):\n",
    "    # loading in TB plasma metabolomics data from tab-delimted file to pandas dataframe\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.rename(columns={df.columns.values[0]: 'metabolite_name'})\n",
    "   \n",
    "    df = df.transpose()\n",
    "    df.columns = df.iloc[0, :]\n",
    "    df = df.iloc[1:, :]\n",
    "    df.index.name = 'sample_id'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df, thresh=0.1):\n",
    "    #drop columns with proportion missing values > threshold\n",
    "    null_allowed = len(df.index) * thresh\n",
    "    null_columns = df.columns.values[df.isnull().sum() > null_allowed]\n",
    "    df = df.drop(columns=null_columns) \n",
    "    #impute remaining nans with minimum value\n",
    "    df = df.apply(lambda x: x.fillna(x.min()), axis=0)\n",
    "    return df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patientmetadata(filename):\n",
    "    # reading in patient metadata\n",
    "    p_df = pd.read_csv(filename)\n",
    "    p_df.columns = p_df.columns.str.lower()\n",
    "    p_df = p_df.set_index('sample_id') \n",
    "    #drop redundant columns\n",
    "    p_df = p_df.drop(columns=[p_df.columns.values[0], 'id'])\n",
    "    \n",
    "    return p_df\n",
    "\n",
    "def load_biochemicaldata(filename):\n",
    "    b_df = pd.read_csv(filename)\n",
    "    b_df.columns = b_df.columns.str.lower()\n",
    "    b_df = b_df.set_index('id')\n",
    "    b_df = b_df.drop(columns=[b_df.columns.values[0]])\n",
    "    return b_df.reset_index()\n",
    "\n",
    "def combine_data(p_df, m_df, b_df):\n",
    "    #join with full dataset\n",
    "    m_df = m_df.set_index('sample_id').join(p_df)\n",
    "    #rename columns with biologically meaningful metabolite names\n",
    "    b_dict = dict(zip(b_df['id'], b_df['biochemical']))\n",
    "    m_df = m_df.rename(b_dict, axis='columns')\n",
    "    m_df = m_df.reset_index()\n",
    "    #consolidate duplicated metabolites as maximum value across duplicates\n",
    "    m_df_nodup = m_df.loc[:, ~m_df.columns.duplicated()]\n",
    "    for dup in m_df.columns[m_df.columns.duplicated()].unique():\n",
    "        temp = m_df[dup]\n",
    "        m_df_nodup[dup] = temp.max(axis=1)\n",
    "    return m_df_nodup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def standardize_data(f_vals):\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # applying standardization \n",
    "    scaler = preprocessing.QuantileTransformer()#StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(f_vals)\n",
    "    \n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(f_vals, features, l_vals, labels):\n",
    "    df = pd.concat([pd.DataFrame(data=l_vals, columns=labels), \n",
    "                    pd.DataFrame(data=f_vals, columns=features)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_PCA(data, l_vals, labels, save=False, ncomp=10):\n",
    "# computing principal components\n",
    "    from sklearn import decomposition\n",
    "\n",
    "    pcaAbs = decomposition.PCA(n_components=ncomp)\n",
    "    data_PCA = pcaAbs.fit_transform(data)\n",
    "    \n",
    "    pc_cols = ['PC ' + str(i) for i in np.arange(1, ncomp + 1)]\n",
    "    df_PCA = make_df(data_PCA, pc_cols, l_vals, labels)\n",
    "    \n",
    "    #Plot explained variance by number of components\n",
    "    var_exp = pcaAbs.explained_variance_ratio_\n",
    "    fig_ve, ax_ve = plt.subplots(1, 1)\n",
    "    sns.lineplot(x=(np.arange(len(var_exp)) + 1), y=np.cumsum(var_exp), ax=ax_ve)\n",
    "    plt.xlabel('PCA component number')\n",
    "    plt.ylabel('Cumulative variance ratio')\n",
    "    if save:\n",
    "        plt.savefig('variance-exp.png', bbox_inches='tight', pad_inches=0.5)\n",
    "    \n",
    "    fig_pca, ax_pca = plt.subplots(1, 1)\n",
    "    sns.scatterplot(x='PC 1', y='PC 2', data=df_PCA, hue='group', ax=ax_pca)\n",
    "    \n",
    "    return df_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p-value on continuous data, selecting appropriate test based on normality & equal variance tests\n",
    "def significanceTest(ctrl, case, alpha_normal=0.05):\n",
    "    try:\n",
    "        _, p_normal_ctrl = sp.stats.normaltest(ctrl, nan_policy='omit')\n",
    "        _, p_normal_case = sp.stats.normaltest(case, nan_policy='omit')\n",
    "    except:\n",
    "        p_normal_ctrl = 1 \n",
    "        p_normal_case = 1\n",
    "        \n",
    "    if (np.any(p_normal_ctrl < alpha_normal) and np.any(p_normal_case < alpha_normal)):\n",
    "        _, p_var = sp.stats.bartlett(ctrl, case)\n",
    "        _, p_diff = sp.stats.ttest_ind(ctrl, case, nan_policy='omit', equal_var=(p_var < alpha_normal))\n",
    "    else:\n",
    "        _, p_diff = sp.stats.ranksums(ctrl, case)\n",
    "    \n",
    "    return p_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significantMetabolites(ctrl, case, features, labels, alpha_normal=0.05, alpha_diff=0.05):\n",
    "    pvals = []\n",
    "    logfc = []\n",
    "    for metab in features:\n",
    "        metab_ctrl = ctrl[metab].values \n",
    "        metab_case = case[metab].values\n",
    "        if (len(metab_ctrl.shape) > 1):\n",
    "            display(metab)\n",
    "        p_diff = significanceTest(metab_ctrl, metab_case, alpha_normal=alpha_normal)\n",
    "        pvals.append(p_diff)\n",
    "        fc = np.mean(metab_case) / np.mean(metab_ctrl) #not matched in any way\n",
    "        logfc.append(np.log2(fc))\n",
    "    padj = multi.multipletests(pvals, alpha=alpha_diff, method='fdr_bh')\n",
    "    \n",
    "    significant = pd.DataFrame({'metabolite' : metabolites, 'log2fc' : logfc,\n",
    "                                'p' :  pvals, 'q' : padj[1]})\n",
    "    \n",
    "    return significant.sort_values(by='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = ['measurements_plasma_full.csv', 'measurements_serum_full.csv', 'measurements_plasmarpmi_full.csv']\n",
    "all_df = []\n",
    "for file in all_files:\n",
    "    temp_df = impute(load_metabolomics(file))\n",
    "    index = temp_df.index\n",
    "    temp_df = pd.DataFrame(data=standardize_data(temp_df), columns=temp_df.columns)\n",
    "    temp_df.index = index\n",
    "    all_df.append(temp_df)\n",
    "    \n",
    "full_df   = pd.concat(all_df, sort=False).reset_index()\n",
    "patient_df = load_patientmetadata('full_unblinded_metadata_with_smoking_tst.csv')\n",
    "chem_df = load_biochemicaldata('biochemicals_full_list_4.csv')\n",
    "full_df = combine_data(patient_df, full_df, chem_df)\n",
    "full_df.to_csv('standardized_TB_metabolomes.csv')\n",
    "\n",
    "labels = list(patient_df)\n",
    "features = [x for x in list(full_df.columns) if x not in labels]\n",
    "f_vals = full_df.loc[:, features].values\n",
    "l_vals = full_df.loc[:, labels].values\n",
    "\n",
    "# displaying shape and first few data entries\n",
    "print('The shape of our data matrix is: ', full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()#['hydroxy-CMPF*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOW WELL DO SAMPLE PREPS CORRELATE?\n",
    "#Extract donors for which there are multiple sample types at a given timepoint\n",
    "dup_df = full_df[full_df.groupby(['donor_id', 'timepoint'])['sample_type'].transform('nunique') > 1] #ends up being only paired\n",
    "#For each donor at each timepoint, calculate a correlation coefficient\n",
    "dup_groups = dup_df.groupby(['donor_id', 'timepoint'])\n",
    "\n",
    "corr = []\n",
    "sig = []\n",
    "donors = []\n",
    "times = []\n",
    "sample_types = []\n",
    "for (donor, time), group in dup_groups:\n",
    "    sample_types.append(group['sample_type'].values)\n",
    "    donors.append(donor)\n",
    "    times.append(time)\n",
    "    \n",
    "    shared_features = group[features[1:]].dropna(axis=1).T #drop columns that are not shared\n",
    "    corr_temp, sig_temp = sp.stats.pearsonr(shared_features.values[:, 0], shared_features.values[:, 1])\n",
    "    corr.append(corr_temp)\n",
    "    sig.append(sig_temp)\n",
    "\n",
    "corr_df = pd.DataFrame({'donor' : donors, 'timepoint' : times, 'sample_types' : sample_types, \n",
    "                        'Pearson correlation' : corr, 'p value' : sig, 'q value' : multi.multipletests(sig, method='fdr_bh')[1]})\n",
    "display(corr_df)\n",
    "#Result: all sample preps correlate significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT METABOLITES DIFFER SIGNIFICANTLY?\n",
    "#Bin by time, to show when we can start detecting changes in the bulk population\n",
    "full_df['time_bin'] = np.floor(np.abs(full_df['time_to_tb'] / 4)) #6 month increments\n",
    "met_tp = []\n",
    "for (timepoint), group in full_df.groupby(['time_bin']):\n",
    "    #group = group\n",
    "    ctrl = group[group['group'].str.contains('control')][features[1:]].dropna(axis=1)\n",
    "    case = group[group['group'].str.contains('case')][features[1:]].dropna(axis=1)\n",
    "    \n",
    "    all_metabs = significantMetabolites(ctrl, case, list(ctrl), labels)\n",
    "    sig_metabs = all_metabs[all_metabs['q'] <= 0.05]\n",
    "    #print('Timepoint : ' + str(timepoint))\n",
    "    #display(sig_metabs)\n",
    "    met_tp.append(all_metabs)\n",
    "    \n",
    "#Result: We only see a large number of significant metabolites <6 months to TB\n",
    "display(met_tp[0])\n",
    "#Plot these for all individuals, color by site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_df['hydroxy-CMPF*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_tp_site = []\n",
    "near_tb = full_df[full_df['time_bin'] == 0]\n",
    "for (site), group in near_tb.groupby(['site']):\n",
    "    ctrl = group[group['group'].str.contains('control')][features].dropna(axis=1)\n",
    "    case = group[group['group'].str.contains('case')][features].dropna(axis=1)\n",
    "    \n",
    "    all_metabs = significantMetabolites(ctrl, case, list(ctrl), labels)\n",
    "    sig_metabs = all_metabs[all_metabs['q'] <= 0.05]\n",
    "    #print('Timepoint : ' + str(timepoint))\n",
    "    display(sig_metabs)\n",
    "    met_tp_site.append(all_metabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(met_tp[0].sort_values(by='log2fc'))\n",
    "display(met_tp[0].sort_values(by='q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT METABOLITES CORRELATE WITH RISK? \n",
    "#analyzing separately by sample type (as broad as possible, color by location)\n",
    "#spearman correlation with progressor status (y/n)\n",
    "#pearson correlation with time to tb (metabolite-by-metabolite) \n",
    "#for a select few, show ones that go up, down, etc. relative to controls\n",
    "\n",
    "#test on downsampled set\n",
    "sig_metabs = met_tp[0][met_tp[0]['q'] <= 0.05]['metabolite'].values\n",
    "metabs_to_test = met_tp[0]['metabolite'].values\n",
    "#print(sig_metabs)\n",
    "all_case = full_df[full_df['group'].str.contains('case')]\n",
    "\n",
    "corr = []\n",
    "sig = []\n",
    "for metab in metabs_to_test:\n",
    "    metab_values = all_case[metab] \n",
    "    tb_time = all_case['time_to_tb']\n",
    "    \n",
    "    corr_temp, sig_temp = sp.stats.spearmanr(metab_values, tb_time, nan_policy='omit')\n",
    "    corr.append(corr_temp)\n",
    "    sig.append(sig_temp)\n",
    "    \n",
    "corr_df = pd.DataFrame({'metab' : metabs_to_test, 'Pearson correlation' : corr, \n",
    "                        'p value' : sig, 'q value' : multi.multipletests(sig, method='fdr_bh')[1]})\n",
    "display(corr_df.sort_values(by='q value'))\n",
    "#In lumped analysis, there are some metabs that correlate with time to TB\n",
    "#Venn diagram of significant metabolites that correlate with time to TB\n",
    "\n",
    "#Progressor vs. control spearman R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT'S THE SIGNAL TO NOISE RATIO?\n",
    "#Within same individual, how does the metabolite change over time? (Means of std. dev, Pearson correlation)\n",
    "#Identifying highly variable metabolites\n",
    "#Pearson correlation between individuals in the same \"case-control\" match\n",
    "\n",
    "all_ctrl = full_df[full_df['group'].str.contains('control')]\n",
    "\n",
    "for (donor), group in all_ctrl.groupby('donor_id'):\n",
    "    shared_features = group[features].dropna(axis=1).T #drop columns that are not shared\n",
    "    \n",
    "    if (shared_features.shape[1] > 1):\n",
    "        corr_temp, sig_temp = sp.stats.pearsonr(shared_features.values[1:, 0], shared_features.values[1:, 1])\n",
    "        display(corr_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRASHES MY NOTEBOOK WHEN I GO BY DONOR LOL CRYING\n",
    "#metab_var = all_ctrl.groupby('donor_id').apply(lambda x: np.mean(x) / np.std(x))\n",
    "#display(metab_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSEA but for metabolites???? (Do we need this?)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_pred(data, features):\n",
    "#Support vector machine\n",
    "#classify  case vs. controls (vanilla version)\n",
    "# ways to improve:\n",
    "# i) Hyperparmeter serach ii) feature selection (serach for best threshold value) iii) test models\n",
    "# using different time points (currently just takes all case and control data) iv) qualitative data?\n",
    "# v) how much data do we need in train vs. test (+validation)\n",
    "\n",
    "    # finding assigning labels to control (0) and case (1) subjects\n",
    "    caseControlLabels = []\n",
    "    for case_control in data['group']:\n",
    "        if case_control == 'control':\n",
    "            caseControlLabels.append(0)\n",
    "        else:\n",
    "            caseControlLabels.append(1)\n",
    "\n",
    "    # extracting metabolite names -- these will be the features\n",
    "    metabolites = []\n",
    "    for feat in features:\n",
    "        if 'M.' in feat:\n",
    "            metabolites.append(feat)\n",
    "\n",
    "    # data matrix will only consist of metabolite features (excluding other qualitative data)\n",
    "    # dropping rows with nan\n",
    "    fullSVM_df = data.loc[:,metabolites[0]:metabolites[-1]].dropna(axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fullSVM_df, caseControlLabels, test_size=0.33, random_state=42)\n",
    "\n",
    "    # hinge loss for linear SVM; L1 for sparsity; don't shuffle data each epoch; don't bias classes; # fairly strong bias for L1\n",
    "    clf = linear_model.SGDClassifier(loss=\"hinge\", penalty=\"l1\", shuffle=False, class_weight=\"balanced\" , alpha=0.001)\n",
    "    featSelection = SelectFromModel(clf, threshold=0.01) # threshold for feature selection \n",
    "    model = Pipeline([\n",
    "                      ('fs', featSelection), \n",
    "                      ('clf', clf), \n",
    "                    ])\n",
    "    model.fit(X_train, y_train)# train model\n",
    "    \n",
    "    # feature weightings\n",
    "    SVMFeat_df = pd.DataFrame([model.named_steps[\"clf\"].coef_[0]], \n",
    "                                    columns=fullSVM_df.columns[model.named_steps[\"fs\"].get_support()])\n",
    "    # calculating model accurarcy\n",
    "    modelAccuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    # ROC calculation\n",
    "    y_score = model.predict(X_test)\n",
    "    y_score = model.decision_function(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # plotting ROC\n",
    "    lw = 2\n",
    "    plt.figure(dpi=400, figsize=(3,3))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('SVM ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # precision recall\n",
    "    plt.figure(dpi=400, figsize=(3,3))\n",
    "    averagePrecision = average_precision_score(y_test, y_score)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "              averagePrecision))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
