{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import collections\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import statsmodels.stats.multitest as multi\n",
    "\n",
    "sns.set_style('white')\n",
    "pd.options.display.float_format = '{:,.7f}'.format\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "\n",
    "#set for reproducibility\n",
    "#check current working directory\n",
    "\n",
    "if (os.getcwd().split('\\\\')[-1] != 'data'):\n",
    "    os.chdir('.\\data')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metabolomics(filename):\n",
    "    # loading in TB plasma metabolomics data from tab-delimted file to pandas dataframe\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.rename(columns={df.columns.values[0]: 'metabolite_name'})\n",
    "   \n",
    "    df = df.transpose()\n",
    "    df.columns = df.iloc[0, :]\n",
    "    df = df.iloc[1:, :]\n",
    "    df.index.name = 'sample_id'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df, thresh=0.1):\n",
    "    #drop columns with proportion missing values > threshold\n",
    "    null_allowed = len(df.index) * thresh\n",
    "    null_columns = df.columns.values[df.isnull().sum() > null_allowed]\n",
    "    df = df.drop(columns=null_columns) \n",
    "    #impute remaining nans with minimum value\n",
    "    df = df.apply(lambda x: x.fillna(x.min()), axis=0)\n",
    "    return df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patientmetadata(filename, m_df):\n",
    "    # reading in patient metadata\n",
    "    p_df = pd.read_csv(filename)\n",
    "    p_df.columns = p_df.columns.str.lower()\n",
    "    p_df = p_df.set_index('sample_id')\n",
    "    #drop redundant columns\n",
    "    p_df = p_df.drop(columns=[p_df.columns.values[0], 'id'])\n",
    "    #join with full dataset\n",
    "    m_df = m_df.set_index('sample_id').join(p_df)\n",
    "\n",
    "    return m_df.reset_index(), p_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def standardize_data(f_vals):\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # applying standardization \n",
    "    scaler = preprocessing.QuantileTransformer()#StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(f_vals)\n",
    "    \n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(f_vals, features, l_vals, labels):\n",
    "    df = pd.concat([pd.DataFrame(data=l_vals, columns=labels), \n",
    "                    pd.DataFrame(data=f_vals, columns=features)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_PCA(data, l_vals, labels, save=False, ncomp=10):\n",
    "# computing principal components\n",
    "    from sklearn import decomposition\n",
    "\n",
    "    pcaAbs = decomposition.PCA(n_components=ncomp)\n",
    "    data_PCA = pcaAbs.fit_transform(data)\n",
    "    \n",
    "    pc_cols = ['PC ' + str(i) for i in np.arange(1, ncomp + 1)]\n",
    "    df_PCA = make_df(data_PCA, pc_cols, l_vals, labels)\n",
    "    \n",
    "    #Plot explained variance by number of components\n",
    "    var_exp = pcaAbs.explained_variance_ratio_\n",
    "    fig_ve, ax_ve = plt.subplots(1, 1)\n",
    "    sns.lineplot(x=(np.arange(len(var_exp)) + 1), y=np.cumsum(var_exp), ax=ax_ve)\n",
    "    plt.xlabel('PCA component number')\n",
    "    plt.ylabel('Cumulative variance ratio')\n",
    "    if save:\n",
    "        plt.savefig('variance-exp.png', bbox_inches='tight', pad_inches=0.5)\n",
    "    \n",
    "    fig_pca, ax_pca = plt.subplots(1, 1)\n",
    "    sns.scatterplot(x='PC 1', y='PC 2', data=df_PCA, hue='group', ax=ax_pca)\n",
    "    \n",
    "    return df_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p-value on continuous data, selecting appropriate test based on normality & equal variance tests\n",
    "def significanceTest(ctrl, case, alpha_normal=0.05):\n",
    "    try:\n",
    "        _, p_normal_ctrl = sp.stats.normaltest(ctrl, nan_policy='omit')\n",
    "        _, p_normal_case = sp.stats.normaltest(case, nan_policy='omit')\n",
    "    except:\n",
    "        p_normal_ctrl = 1 \n",
    "        p_normal_case = 1\n",
    "    #print(metab, p_normal_ctrl, p_normal_case)\n",
    "    \n",
    "    if (p_normal_ctrl < alpha_normal and p_normal_case < alpha_normal):\n",
    "        _, p_var = sp.stats.bartlett(ctrl, case)\n",
    "        _, p_diff = sp.stats.ttest_ind(ctrl, case, nan_policy='omit', equal_var=(p_var < alpha_normal))\n",
    "    else:\n",
    "        _, p_diff = sp.stats.ranksums(ctrl, case)\n",
    "    \n",
    "    return p_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significantMetabolites(ctrl, case, features, labels, alpha_normal=0.05, alpha_diff=0.05):\n",
    "    metabolites = features[:-1]\n",
    "    #print(metabolites)\n",
    "    \n",
    "    pvals = []\n",
    "\n",
    "    for metab in metabolites:\n",
    "        metab_ctrl = ctrl[metab].values \n",
    "        metab_case = case[metab].values\n",
    "\n",
    "        p_diff = significanceTest(metab_ctrl, metab_case, alpha_normal=alpha_normal)\n",
    "        pvals.append(p_diff)\n",
    "        \n",
    "    padj = multi.multipletests(pvals, alpha=alpha_diff, method='fdr_bh')\n",
    "\n",
    "    significant = pd.DataFrame({'metabolite' : metabolites, 'p' :  pvals, 'q' : padj[1]})\n",
    "    display(significant.sort_values(by='p'))\n",
    "    \n",
    "    return significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = ['measurements_plasma_full.csv', 'measurements_serum_full.csv', 'measurements_plasmarpmi_full.csv']\n",
    "all_df = []\n",
    "for file in all_files:\n",
    "    temp_df = impute(load_metabolomics(file))\n",
    "    index = temp_df.index\n",
    "    temp_df = pd.DataFrame(data=standardize_data(temp_df), columns=temp_df.columns)\n",
    "    temp_df.index = index\n",
    "    all_df.append(temp_df)\n",
    "    \n",
    "full_df   = pd.concat(all_df, sort=False).reset_index()\n",
    "full_df, patient_df = load_patientmetadata('full_unblinded_metadata_with_smoking_tst.csv', full_df)\n",
    "full_df.to_csv('standardized_TB_metabolomes.csv')\n",
    "\n",
    "labels = list(patient_df)\n",
    "features = [x for x in full_df.columns.to_list() if x not in labels]\n",
    "f_vals = full_df.loc[:, features].values\n",
    "l_vals = full_df.loc[:, labels].values\n",
    "\n",
    "# displaying shape and first few data entries\n",
    "print('The shape of our data matrix is: ', full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOW WELL DO SAMPLE PREPS CORRELATE?\n",
    "#Extract donors for which there are multiple sample types at a given timepoint\n",
    "dup_df = full_df[full_df.groupby(['donor_id', 'timepoint'])['sample_type'].transform('nunique') > 1] #ends up being only paired\n",
    "#For each donor at each timepoint, calculate a correlation coefficient\n",
    "dup_groups = dup_df.groupby(['donor_id', 'timepoint'])\n",
    "\n",
    "corr = []\n",
    "sig = []\n",
    "donors = []\n",
    "times = []\n",
    "sample_types = []\n",
    "for (donor, time), group in dup_groups:\n",
    "    sample_types.append(group['sample_type'].values)\n",
    "    donors.append(donor)\n",
    "    times.append(time)\n",
    "    \n",
    "    shared_features = group[features].dropna(axis=1).T #drop columns that are not shared\n",
    "    corr_temp, sig_temp = sp.stats.pearsonr(shared_features.values[:, 0], shared_features.values[:, 1])\n",
    "    corr.append(corr_temp)\n",
    "    sig.append(sig_temp)\n",
    "\n",
    "corr_df = pd.DataFrame({'donor' : donors, 'timepoint' : times, 'sample_types' : sample_types, \n",
    "                        'Pearson correlation' : corr, 'p value' : sig, 'q value' : multi.multipletests(sig, method='fdr_bh')[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(group['sample_type'].values)\n",
    "display(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT METABOLITES DIFFER SIGNIFICANTLY?\n",
    "#--what's the best way to do this analysis? \n",
    "## analyze a bunch of different resolutions and see what the overlap is\n",
    "for (sample_type, timepoint), group in full_df.groupby(['sample_type', 'time_to_tb']):\n",
    "    #group = group\n",
    "    ctrl = group[group['group'].str.contains('control')][features].dropna(axis=1)\n",
    "    case = group[group['group'].str.contains('case')][features].dropna(axis=1)\n",
    "    \n",
    "    significant = significantMetabolites(ctrl, case, list(ctrl), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT METABOLITES CORRELATE WITH RISK? \n",
    "#analyzing separately by sample type (as broad as possible, color by location)\n",
    "#spearman correlation with progressor status (y/n)\n",
    "#pearson correlation with time to tb (metabolite-by-metabolite) \n",
    "#for a select few, show ones that go up, down, etc. relative to controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT'S THE SIGNAL TO NOISE RATIO?\n",
    "#Within same individual, how does the metabolite change over time? (Means of std. dev, Pearson correlation)\n",
    "#Identifying highly variable metabolites\n",
    "#Pearson correlation between individuals in the same \"case-control\" match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSEA but for metabolites???? (Do we need this?)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
